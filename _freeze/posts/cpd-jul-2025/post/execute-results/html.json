{
  "hash": "ca0bb3d9b5f19e5fee1a435a3538e789",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"CPD for July 2025\"\ndescription: \"A grab bag of stuff\"\nauthor:\n  - name: Jim Gardner\n    url: https://jimgar.github.io/jimgar/\ndate: 2025-07-11\ncategories: [Python, Advent of Code, Advent of Code 2024, Polars, uv]\nengine: knitr\neval: false\ndraft: false\n---\n\n\n\nMy team (Data Science) at Smart Data Foundry have started to spend an hour of CPD time together once a week. July is our first month doing it, and I figured I would use it as an excuse to blog a little bit.\n\n## 2025-07-10\n\n### Advent of Code 2024 Day 1, Part 1\n\nI [did AoC Day 1 last year](https://github.com/jimgar/advent-of-code/blob/main/2024/01.R) and challenged myself to only use base R. We figured this would be a nice little problem to start our sessions with.\n\nThis time, I wanted to try using Python. I'm kinda enjoying the language now, and between boot.dev and work am using it more than R. But! Instead of the standard library, I wanted to try using Polars and compare the solution to my base R one. Recently I've used Polars a fair bit for work but the operations I needed have been **very** simple. My spidey senses told me that if I used Polars then I'd end up learning something new.\n\nI also used uv, the hot new Python project/package manager, to start the project off:\n\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nuv init\nuv venv\nsource venv/bin/activate\nuv add ruff\nuv add polars\n```\n:::\n\n\n\nThat creates a few files, including a `uv.lock` and `pyproject.toml`. These essentially contain metadata such as the Python and package versions. The files can be copied into another project, or a container image, to install the same version of Python and packages with a simple `uv sync`.\n\n#### Solution\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport polars as pl\n\n\ndf = (\n    pl.read_csv(\"aoc_day1/input.txt\", has_header=False)\n    .with_columns(\n        column_1=pl.col(\"column_1\")\n        .str.split_exact(\"   \", 1)\n        .struct.rename_fields([\"l\", \"r\"])\n    )\n    .unnest(\"column_1\")\n)\n\ndf = df.with_columns(\n    l=pl.col(\"l\").cast(pl.Int32).sort(),\n    r=pl.col(\"r\").cast(pl.Int32).sort(),\n)\n\nres = df.select((pl.col(\"r\") - pl.col(\"l\")).abs().sum()).item()\n\nprint(res)\n```\n:::\n\n\n\nComparing to the R version of the code:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- read.table(\"2024/01-input.txt\", col.names = c(\"l\", \"r\"))\n\ndf$l <- sort(df$l)\ndf$r <- sort(df$r)\n\nsum(abs(df$r - df$l))\n```\n:::\n\n\n\nIt's more verbose. Most of that comes down to fundamental differences between base R and Polars. In this case - which is common - the R solution uses functions. We also benefit from data.frames being native data structures, and `read.table` which does two things for us: identifies the delimiter (three whitespace characters), and infers the types correctly (numeric).\n\nThe Polars solution is fiddlier. The csv parser can only take a single byte delimiter, meaning no regex, no multiple whitespace. I had to look this up, because it seemed frankly idiotic for a modern data processing package, and read the author saying they did this on purpose: It puts speed before anything else, and speed is at the heart of Polars. Doing anything else would go against that. Not so idiotic after all!!!\n\nAnyway this leads to extra processing because the table gets read as a single string column. When it gets split, it becomes a Polars **[struct](https://docs.pola.rs/user-guide/expressions/structs/)**. I didn't know what a struct actually was, though I saw the name appear in the user guide as I hacked on projects for work. Turns out they're essentially a typed dictionary.\n\n`str.split_exact()` returns \"a struct of `n+1` fields\". So the result of \n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nsplit_exact(\"   \", 1)\n```\n:::\n\n\n\non a row of data like `50123   10023` in a column called `column_1` is\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n{\"column_1\": \"50123\", \"column_2\": \"10023\"}\n```\n:::\n\n\n\nEvery row gets a dict like that, which taken together are the values in the struct.\n\nCalling `unnest()` splits a struct into columns. The column names come from whatever the fields are called.\n\nSo, all in all, it's a little bit like an R list-column situation.\n\nOnce the columns were unnested they were still string and have to be cast to the correct integer type. And then finally we get to do the calculation, which also looks way more verbose to me. One benefit of chaining methods is that you get to see what happens sequentially. That's not the case with nested function calls, like in the R code. I don't like the amount of noise in having to refer to columns with `pl.col(\"l\")`, but it's not that bad in this case to be honest.\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}