{
  "hash": "d11e7232f689bbf10e32c84207e9b890",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"CPD for July 2025\"\ndescription: \"A grab bag of stuff\"\nauthor:\n  - name: Jim Gardner\n    url: https://jimgar.github.io/jimgar/\ndate: 2025-07-11\ncategories: [Python, Advent of Code, Advent of Code 2024, Polars, uv]\nengine: knitr\neval: false\ndraft: false\n---\n\n\n\nMy team (Data Science) at Smart Data Foundry have started to spend an hour of CPD time together once a week. July is our first month doing it, and I figured I would use it as an excuse to blog a little bit.\n\n## 2025-07-10\n\n### Advent of Code 2024 Day 1, Part 1\n\nI [did AoC Day 1 last year](https://github.com/jimgar/advent-of-code/blob/main/2024/01.R) and challenged myself to only use base R. We figured this would be a nice little problem to start our sessions with.\n\nThis time, I wanted to try using Python. I'm kinda enjoying the language now, and between boot.dev and work am using it more than R. But! Instead of the standard library, I wanted to try using Polars and compare the solution to my base R one. Recently I've used Polars a fair bit for work but the operations I needed have been **very** simple. My spidey senses told me that if I used Polars then I'd end up learning something new.\n\nI also used uv, the hot new Python project/package manager, to start the project off:\n\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nuv init\nuv venv\nsource venv/bin/activate\nuv add ruff\nuv add polars\n```\n:::\n\n\n\nThat creates a few files, including a `uv.lock` and `pyproject.toml`. These essentially contain metadata such as the Python and package versions. The files can be copied into another project, or a container image, to install the same version of Python and packages with a simple `uv sync`.\n\n#### Solution\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport polars as pl\n\n\ndf = (\n    pl.read_csv(\"aoc_day1/input.txt\", has_header=False)\n    .with_columns(\n        column_1=pl.col(\"column_1\")\n        .str.split_exact(\"   \", 1)\n        .struct.rename_fields([\"l\", \"r\"])\n    )\n    .unnest(\"column_1\")\n)\n\ndf = df.with_columns(\n    l=pl.col(\"l\").cast(pl.Int32).sort(),\n    r=pl.col(\"r\").cast(pl.Int32).sort(),\n)\n\nres = df.select((pl.col(\"r\") - pl.col(\"l\")).abs().sum()).item()\n\nprint(res)\n```\n:::\n\n\n\nComparing to the R version of the code:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- read.table(\"2024/01-input.txt\", col.names = c(\"l\", \"r\"))\n\ndf$l <- sort(df$l)\ndf$r <- sort(df$r)\n\nsum(abs(df$r - df$l))\n```\n:::\n\n\n\nIt's more verbose. Most of that comes down to fundamental differences between base R and Polars. In this case - which is common - the R solution uses functions. We also benefit from data.frames being native data structures, and `read.table` which does two things for us: identifies the delimiter (three whitespace characters), and infers the types correctly (numeric).\n\nThe Polars solution is fiddlier. The csv parser can only take a single byte delimiter, meaning no regex, no multiple whitespace. I had to look this up, because it seemed frankly idiotic for a modern data processing package, and read the author saying they did this on purpose: It puts speed before anything else, and speed is at the heart of Polars. Doing anything else would go against that. Not so idiotic after all!!!\n\nAnyway this leads to extra processing because the table gets read as a single string column. When it gets split, it becomes a Polars **[struct](https://docs.pola.rs/user-guide/expressions/structs/)**. I didn't know what a struct actually was, though I saw the name appear in the user guide as I hacked on projects for work. Turns out they're essentially a typed dictionary.\n\n`str.split_exact()` returns \"a struct of `n+1` fields\". So the result of \n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nsplit_exact(\"   \", 1)\n```\n:::\n\n\n\non a row of data like `50123   10023` in a column called `column_1` is\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n{\"column_1\": \"50123\", \"column_2\": \"10023\"}\n```\n:::\n\n\n\nEvery row gets a dict like that, which taken together are the values in the struct.\n\nCalling `unnest()` splits a struct into columns. The column names come from whatever the fields are called.\n\nSo, all in all, it's a little bit like an R list-column situation.\n\nOnce the columns were unnested they were still string and have to be cast to the correct integer type. And then finally we get to do the calculation, which also looks way more verbose to me. One benefit of chaining methods is that you get to see what happens sequentially. That's not the case with nested function calls, like in the R code. I don't like the amount of noise in having to refer to columns with `pl.col(\"l\")`, but it's not that bad in this case to be honest.\n\n## 2025-07-17\n\n### Advent of Code 2024 Day 1, Part 2\n\nSame deal as last week, but part two of the problem.\n\n#### Solution\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# df was defined in the week 1 section\n\ncounts = df.select(\"r\").to_series().value_counts()\n\njoined = df.join(counts, how=\"inner\", left_on=\"l\", right_on=\"r\")\n\njoined = joined.with_columns(multiplied=pl.col(\"count\") * pl.col(\"l\"))\n\nsummed = joined.select(\"multiplied\").sum().item()\n\nprint(f\"Part 2 result: {summed}\")\n```\n:::\n\n\n\nComparing to the R version:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# df already defined in the week 1 section\n\nfreq <- as.data.frame(table(df$r[df$r %in% df$l]))\nnames(freq) <- c(\"l\", \"f\")\n\ndf <- merge(df, freq, by = \"l\", all.x = TRUE, sort = FALSE)\n\nsum(df$l * df$f, na.rm = TRUE)\n```\n:::\n\n\n\nIt's a similar approach: Make a frequency table of the values in the right-hand column, join those values onto the original table, multiply the left-hand values against the frequency values and sum up the results.\n\nI couldn't remember what I did in the R version, but my mind went to almost exactly the same places. In both versions I started by making a frequency table, which was trivial. However, in the R version I first filtered the values in the right-hand column by those of the left-hand column, so that the frequency table would be much smaller (there were only 37 shared values between the columns in my dataset). This seemed more efficient for further steps because with a smaller dataframe there will be fewer comparisons to make.\n\nThis was what I wanted to do in the Polars version, too, but it was fiddly. There might be a better way of doing this, but my code ended up looking like:\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nmask = df.select(\"r\").to_series().is_in(df.select(\"l\").to_series())\ncounts = df.select(\"r\").to_series().filter(mask).value_counts()\n```\n:::\n\n\n\nThat's not terrible, but to me the R way of subsetting a vector looks **far** more elegant. Once again Polars is more verbose; I get lost trying to read the repetition of `select` and `to_series` and the brackets. There **is** a `df.to_series()` method, so the code could be streamlined slightly, but it takes a column's position as it's arg, rather than a name. I stand resolute and refuse to do `df.to_series(0)`. That... is opaque bullshit!\n\nInstead, I did an inner join between the two DataFrames. An inner join is a \"filtering join\", where the left-hand df gets filtered by the contents of the right-hand df. And in the R version I did the same thing using `merge()`. \n\nAt this juncture of writing the blog post something quite nice happened. I realise that the R code was doing some redundant things. Because `merge()` does an inner join by default, there was no need to specify `all.x = TRUE`. In fact, it causes the opposite behaviour of what is needed: It's like saying \"Yes, make sure you keep all the rows from the left-hand df, and if you don't mind, add NAs whenever there was no matching row in the right-hand df!\". That's why the subsequent sum requires `na.rm = TRUE`.\n\nIn my opinion the more elegant approach is the Polars version: Don't bother filtering the frequency table first, simply allow the inner join to do its job and filter for you.\n\nWith those changes made, the R code is even slicker (isn't it satisfying to delete things??):\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfreq <- as.data.frame(table(df$r))\nnames(freq) <- c(\"l\", \"f\")\n\ndf <- merge(df, freq, by = \"l\", sort = FALSE)\n\nsum(df$l * df$f)\n```\n:::\n\n\n\nAll in all:\n\n- Humbling to look back at simple problems and try to solve them with a new tool\n- Satisfying to revisit concepts and end up streamlining old code\n- Interesting to observe what **feels nicer to read** as I look at the code. Never underestimate how important feel is to craft\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}